{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\joana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries adn load dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models:\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification # for german\n",
    "from pattern.en import sentiment # for english\n",
    "import nltk # for english\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for english\n",
    "nltk.download('vader_lexicon') # for english\n",
    "from pysentimiento import create_analyzer # for spanish\n",
    "\n",
    "# Accuracy\n",
    "# from utils import evaluate_performance #ACTION: uncomment and put utils function away\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "def evaluate_performance(df, sentiment_column, label_column):\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(df[label_column], df[sentiment_column])\n",
    "\n",
    "    # Find unique predicted sentiment labels\n",
    "    unique_predicted = df[sentiment_column].unique()\n",
    "\n",
    "    # Assign true and predicted labels\n",
    "    true_labels = df[label_column]\n",
    "    predicted_labels = df[sentiment_column]\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Convert the confusion matrix to a DataFrame for better visualization\n",
    "    labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    return accuracy, unique_predicted, cm_df, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More sentiment models and their accuracy per language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "TODO\n",
    "\n",
    "full text\n",
    "extracted sentence\n",
    "\n",
    "full condensed\n",
    "extracted sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labeled data: d1.1\n",
    "Explanation why we used d1.1 and that is already with extracted sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled CSV files into a DataFrame\n",
    "df_de_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-de_clean_1-1.csv', sep=';')\n",
    "df_en_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-en_clean_1-1_not101010.csv')\n",
    "df_es_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-es_clean_1-1.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labeled data: data condensed for english\n",
    "Explanation why we used data condensed for english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled CSV file into a DataFrame\n",
    "df_en_con = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-en_clean_con_sen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German\n",
    "For the german data we used one additional model to the multilingual only, because then we focused on clustering.\n",
    "\n",
    "### Model: AutoTokenizer and AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "The model used was trained on 1.834 million German-language samples sepcifically for sentiment classification.\n",
    "https://huggingface.co/oliverguhr/german-sentiment-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  data    player language  \\\n",
      "0    trainer alonso vor den mitgereisten fans in mo...  palacios       de   \n",
      "1    zudem ist die konkurrenzsituation auf der dopp...  palacios       de   \n",
      "2    wie auch palacios sah der defensive mittelfeld...  palacios       de   \n",
      "3    ,,er ist eine option\", erklart alonso, der im ...  palacios       de   \n",
      "4       allerdings waren in andrich und dem argenti...  palacios       de   \n",
      "..                                                 ...       ...      ...   \n",
      "434  vor allem monacos krepin diatta und bayerlever...  hincapie       de   \n",
      "435     vor allem monacos krepin diatta und bayerle...  hincapie       de   \n",
      "436  in der funften minute der nachspielzeit sah ba...  hincapie       de   \n",
      "437  une febrilite deconcertante venant du bayerlev...  hincapie       de   \n",
      "438  sekunden waren gespielt, als andrich nach here...  hincapie       de   \n",
      "\n",
      "              publishedAt    Label sentiment_bert  \n",
      "0    2023-02-24T09:33:31Z      NaN        negativ  \n",
      "1    2023-03-03T21:35:13Z      NaN        negativ  \n",
      "2    2023-03-07T11:34:39Z      NaN        negativ  \n",
      "3    2023-03-08T14:25:18Z      NaN        negativ  \n",
      "4    2023-03-09T19:53:46Z      NaN        negativ  \n",
      "..                    ...      ...            ...  \n",
      "434  2023-02-17T08:56:42Z      NaN        negativ  \n",
      "435  2023-02-17T07:51:22Z      NaN        negativ  \n",
      "436  2023-05-21T19:53:27Z  negativ        negativ  \n",
      "437  2023-05-21T19:30:59Z      NaN        negativ  \n",
      "438  2023-05-12T10:02:12Z      NaN        negativ  \n",
      "\n",
      "[439 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def german_sentiment_model(df):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "    # Create an empty list to store the sentiment scores\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # Iterate over the 'data' column in the DataFrame\n",
    "    for text in df['data']:\n",
    "        # Tokenize the input text\n",
    "        tokens = tokenizer.encode_plus(text, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                       return_tensors=\"pt\")\n",
    "\n",
    "        # Perform the sentiment analysis\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokens)[0]\n",
    "\n",
    "        # Convert logits to predicted label (positive/negative)\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        sentiment = \"positive\" if predicted_label == 1 else \"negativ\"\n",
    "\n",
    "        # Append the sentiment score to the list\n",
    "        sentiment_scores.append(sentiment)\n",
    "\n",
    "    # Add the sentiment scores as a new column in the DataFrame\n",
    "    df['sentiment_bert'] = sentiment_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Perform sentiment analysis on the DataFrame\n",
    "df_de_1 = german_sentiment_model(df_de_1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df_de_1)\n",
    "\n",
    "\n",
    "\n",
    "#ACTION: NOTE FOR JOANA: check whether this makes a new dataframe instead of appending a column. If so: Change & check what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for german bert model\n",
    "TODO and todo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_de_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_de_1_pos_neg = df_de_1[df_de_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for oliverguhr/german-sentiment-bert\n",
      "Accuracy:  0.5  Unique labels:  ['negativ']\n",
      "Confusion matrix: \n",
      "         negativ  positiv\n",
      "negativ       10        0\n",
      "positiv       10        0\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.50      1.00      0.67        10\n",
      "     positiv       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.25      0.50      0.33        20\n",
      "weighted avg       0.25      0.50      0.33        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for oliverguhr/german-sentiment-bert')\n",
    "accuracy_de, unique_predicted_de, confusion_matrix_de, classification_report_de = evaluate_performance(df_de_1_pos_neg, 'sentiment_bert', 'Label')\n",
    "\n",
    "print('Accuracy: ', accuracy_de,' Unique labels: ', unique_predicted_de)\n",
    "\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_de)\n",
    "print('Classification report: ')\n",
    "print(classification_report_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English - d1.1\n",
    "TODO\n",
    "\n",
    "### Why we used the pipeline for the english and spanish models\n",
    "TODO\n",
    "\n",
    "\n",
    "### Model 1: sentiment-analysis from bert-base-uncased\n",
    "TODO\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>player</th>\n",
       "      <th>language</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentiment_bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten if you included the toe-poked volley to te...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-16T23:56:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayerleverkusen took the lead again in the st ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:50:50Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wissam ben yedder levelled straight away from ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:53:59Z</td>\n",
       "      <td>positiv</td>\n",
       "      <td>0.619449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midfielders: leandro paredes (juventus), angel...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T16:40:46Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.622141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>midfielders: rodrigo de paul (atletico madrid)...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T18:17:37Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.622096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data    player language  \\\n",
       "0  ten if you included the toe-poked volley to te...  palacios       en   \n",
       "1  bayerleverkusen took the lead again in the st ...  palacios       en   \n",
       "2  wissam ben yedder levelled straight away from ...  palacios       en   \n",
       "3  midfielders: leandro paredes (juventus), angel...  palacios       en   \n",
       "4  midfielders: rodrigo de paul (atletico madrid)...  palacios       en   \n",
       "\n",
       "            publishedAt    Label  sentiment_bert  \n",
       "0  2023-02-16T23:56:00Z      NaN        0.566346  \n",
       "1  2023-02-23T20:50:50Z      NaN        0.581037  \n",
       "2  2023-02-23T20:53:59Z  positiv        0.619449  \n",
       "3  2023-03-03T16:40:46Z  neutral        0.622141  \n",
       "4  2023-03-03T18:17:37Z  neutral        0.622096  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate model\n",
    "sentiment_classifier_en = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_bert\"\n",
    "df_en_1['sentiment_bert'] = df_en_1['data'].apply(lambda x: sentiment_classifier_en(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Sentiment Intensity Analyzer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>player</th>\n",
       "      <th>language</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentiment_bert</th>\n",
       "      <th>sentiment_pattern</th>\n",
       "      <th>sentiment_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten if you included the toe-poked volley to te...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-16T23:56:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566346</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayerleverkusen took the lead again in the st ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:50:50Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581037</td>\n",
       "      <td>(0.20833333333333334, 0.3)</td>\n",
       "      <td>-0.0516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wissam ben yedder levelled straight away from ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:53:59Z</td>\n",
       "      <td>positiv</td>\n",
       "      <td>0.619449</td>\n",
       "      <td>(0.1, 0.2)</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midfielders: leandro paredes (juventus), angel...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T16:40:46Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.622141</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>midfielders: rodrigo de paul (atletico madrid)...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T18:17:37Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.622096</td>\n",
       "      <td>(0.2, 0.30000000000000004)</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data    player language  \\\n",
       "0  ten if you included the toe-poked volley to te...  palacios       en   \n",
       "1  bayerleverkusen took the lead again in the st ...  palacios       en   \n",
       "2  wissam ben yedder levelled straight away from ...  palacios       en   \n",
       "3  midfielders: leandro paredes (juventus), angel...  palacios       en   \n",
       "4  midfielders: rodrigo de paul (atletico madrid)...  palacios       en   \n",
       "\n",
       "            publishedAt    Label  sentiment_bert           sentiment_pattern  \\\n",
       "0  2023-02-16T23:56:00Z      NaN        0.566346                  (0.0, 0.0)   \n",
       "1  2023-02-23T20:50:50Z      NaN        0.581037  (0.20833333333333334, 0.3)   \n",
       "2  2023-02-23T20:53:59Z  positiv        0.619449                  (0.1, 0.2)   \n",
       "3  2023-03-03T16:40:46Z  neutral        0.622141                  (0.0, 0.0)   \n",
       "4  2023-03-03T18:17:37Z  neutral        0.622096  (0.2, 0.30000000000000004)   \n",
       "\n",
       "   sentiment_nltk  \n",
       "0          0.0000  \n",
       "1         -0.0516  \n",
       "2          0.2263  \n",
       "3          0.0000  \n",
       "4          0.0000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "\n",
    "# Apply sentiment analysis to the \"data\" column and store the sentiment in a new column \"sentiment_nltk\"\n",
    "df_en_1['sentiment_nltk'] = df_en_1['data'].apply(get_sentiment)\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for all english models\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_en_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en_1_pos_neg = df_en_1[df_en_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for bert-base-uncased with two-dimensional labels (positive/negative) \n",
      "Accuracy:  0.7647058823529411  Unique labels:  ['positiv']\n",
      "Confusion matrix: \n",
      "         negativ  positiv\n",
      "negativ        0        4\n",
      "positiv        0       13\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.00      0.00      0.00         4\n",
      "     positiv       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.76        17\n",
      "   macro avg       0.38      0.50      0.43        17\n",
      "weighted avg       0.58      0.76      0.66        17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Temp\\ipykernel_28752\\2779216896.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en_1_pos_neg['sentiment_2_label_bert'] = sentiment_2_labels\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for bert-base-uncased with two-dimensional labels (positive/negative)')\n",
    "\n",
    "# Transform score into two-dimensional label for Performance evaluation\n",
    "sentiment_2_labels = []\n",
    "for score in df_en_1_pos_neg['sentiment_bert']:        \n",
    "    # Convert sentiment score to two-dimensional label (positive/negative)\n",
    "    sentiment_2_label = \"positiv\" if score > 0 else \"negativ\"\n",
    "    sentiment_2_labels.append(sentiment_2_label)\n",
    "df_en_1_pos_neg['sentiment_2_label_bert'] = sentiment_2_labels\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy_en_bert, unique_predicted_en_bert, confusion_matrix_en_bert, classification_report_en_bert = evaluate_performance(df_en_1_pos_neg, 'sentiment_2_label_bert', 'Label')\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy: ', accuracy_en_bert, ' Unique labels: ', unique_predicted_en_bert)\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_en_bert)\n",
    "print('Classification report: ')\n",
    "print(classification_report_en_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for nltk\n",
      "Accuracy:  0.6470588235294118  Unique labels:  ['positiv' 'negativ']\n",
      "Confusion matrix: \n",
      "         negativ  positiv\n",
      "negativ        3        1\n",
      "positiv        5        8\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.38      0.75      0.50         4\n",
      "     positiv       0.89      0.62      0.73        13\n",
      "\n",
      "    accuracy                           0.65        17\n",
      "   macro avg       0.63      0.68      0.61        17\n",
      "weighted avg       0.77      0.65      0.67        17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Temp\\ipykernel_28752\\695854752.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en_1_pos_neg['sentiment_2_label_nltk'] = sentiment_2_labels\n"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for nltk')\n",
    "\n",
    "# Transform score into two-dimensional label for Performance evaluation\n",
    "sentiment_2_labels = []\n",
    "for score in df_en_1_pos_neg['sentiment_nltk']:        \n",
    "    # Convert sentiment score to two-dimensional label (positive/negative)\n",
    "    sentiment_2_label = \"positiv\" if score > 0 else \"negativ\"\n",
    "    sentiment_2_labels.append(sentiment_2_label)\n",
    "df_en_1_pos_neg['sentiment_2_label_nltk'] = sentiment_2_labels\n",
    "\n",
    "accuracy_en_nltk, unique_predicted_en_nltk, confusion_matrix_en_nltk, classification_report_en_nltk = evaluate_performance(df_en_1_pos_neg, 'sentiment_2_label_nltk', 'Label')\n",
    "\n",
    "print('Accuracy: ', accuracy_en_nltk, ' Unique labels: ', unique_predicted_en_nltk)\n",
    "\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_en_nltk)\n",
    "print('Classification report: ')\n",
    "print(classification_report_en_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish\n",
    "TODO?\n",
    "\n",
    "### Model 1: sentiment-analysis from spanish bert: beto-sentiment-analysis\n",
    "TODO\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis\n",
    "Although this model was trained on tweets, not on news data, it is a popular model to use for sentiment analysis of spanish data.\n",
    "The base model is BETO, a BERT model trained on spanish data (for further information: https://github.com/dccuchile/beto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier_es_beto = pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_beto\"\n",
    "df_es_1['sentiment_beto'] = df_es_1['data'].apply(lambda x: sentiment_classifier_es_beto(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_es_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: sentiment-analysis from another spanish bert: bert-base-spanish-wwm-uncased\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier_es_bert = pipeline('sentiment-analysis', model='dccuchile/bert-base-spanish-wwm-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_bert\"\n",
    "df_es_1['sentiment_bert'] = df_es_1['data'].apply(lambda x: sentiment_classifier_es_bert(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_es_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for all spanish models\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_es_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_es_1_pos_neg = df_es_1[df_es_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for beto-sentiment-analysis')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_es_beto = (df_es_1_pos_neg['sentiment_beto'] == df_es_1_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for beto: {:.2f}%\".format(accuracy_es_beto))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for beto:')\n",
    "print(df_es_1_pos_neg['sentiment_beto'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_es_1_pos_neg['Label']\n",
    "predicted_labels = df_es_1_pos_neg['sentiment_beto']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_es_beto = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for beto:\")\n",
    "print(cm_df_es_beto)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_es_beto = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for beto:\")\n",
    "print(report_es_beto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for bert-base-spanish-wwm-uncased')\n",
    "\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_es_bert = (df_es_1_pos_neg['sentiment_bert'] == df_es_1_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for bert: {:.2f}%\".format(accuracy_es_bert))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for bert:')\n",
    "print(df_es_1_pos_neg['sentiment_bert'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_es_1_pos_neg['Label']\n",
    "predicted_labels = df_es_1_pos_neg['sentiment_bert']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_es_bert = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for bert:\")\n",
    "print(cm_df_es_bert)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_es_bert = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for beto:\")\n",
    "print(report_es_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English: data condensed\n",
    "TODO\n",
    "\n",
    "### Model 1: sentiment-analysis from bert-base-uncased\n",
    "TODO\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loaded above: sentiment_classifier_en = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_bert\"\n",
    "df_en_con['sentiment_bert'] = df_en_con['data'].apply(lambda x: sentiment_classifier_en(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_con.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for english condensed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_en_con.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en_con_pos_neg = df_en_con[df_en_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for bert-base-uncased on english condensed')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_en_con_bert = (df_en_con_pos_neg['sentiment_bert'] == df_en_con_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for bert: {:.2f}%\".format(accuracy_en_con_bert))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for bert:')\n",
    "print(df_en_con_pos_neg['sentiment_bert'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_en_con_pos_neg['Label']\n",
    "predicted_labels = df_en_con_pos_neg['sentiment_bert']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_en_con_bert = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for bert:\")\n",
    "print(cm_df_en_con_bert)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_en_con_bert = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for bert:\")\n",
    "print(report_en_con_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
