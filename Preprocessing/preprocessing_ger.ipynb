{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import remove_similar_rows_per_player, find_lines_with_player, del_patterns, map_emoji_to_description, remove_accents, name_wordgroups\n",
    "import emoji\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/data_files/all_data_v3.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Transformation (with Pattern deletion)\n",
    "The first transformation focus on the deletion of Patterns to clean the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out German data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the German data and reindex\n",
    "df_ger = df[df[\"language\"] == \"de\"]\n",
    "df_ger = df_ger.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similiar rows (The Function is imported from utils on top)\n",
    "df_before = remove_similar_rows_per_player(df_ger, df_ger['player'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform Hincapié to Hincapie\n",
    "df_before.loc[df_before['player'] == 'Piero Hincapié', 'player'] = 'Piero Hincapie'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into lower case\n",
    "data_lower = df_before.copy()\n",
    "\n",
    "data_lower['data'] = data_lower['data'].str.lower()\n",
    "data_lower['player'] = data_lower['player'].str.lower()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delte Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete content patterns\n",
    "data_wo_pattern = data_lower.copy()\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r\"^{\\'content\\': \\'\", \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ['nutze kicker', \n",
    "           'mit dem', \n",
    "           'nur €2,49', \n",
    "           'bereits pur-abonnent?', \n",
    "           'alle antworten', \n",
    "           'hinweis zur verarbeitung', \n",
    "           'werbung','olympia-verlags', \n",
    "           'bild', 'g+j medien gmbh', \n",
    "           'services',\n",
    "           'kurz-link dieses artikels lautet:', \n",
    "           'http://epaper.welt.de',\n",
    "           'stephan von nocks',\n",
    "           'www.faz.net',\n",
    "           'mcfit mitgliedschaft',\n",
    "           'fitx-vertrag',\n",
    "           'kündigeneasyfitness',\n",
    "           'proteinbedarf',\n",
    "           'fitnessland',\n",
    "           'kündigeneasyfitness',\n",
    "           'trainingspuls berechnen',\n",
    "           'pulsrechner',\n",
    "           'fitseveneleven-mitgliedschaft',\n",
    "           'alkoholabbau & promille',\n",
    "           'index.promillerechner',\n",
    "           'mitgliedschaft kündigen',\n",
    "           'promillerechner',\n",
    "           'ihr body-mass-index',\n",
    "           'bmi rechner',\n",
    "           'kalorienrechner',\n",
    "           'grundumsatz & kalorienbedarf',\n",
    "           'partnerangebote',\n",
    "           'newsletter',\n",
    "           'journalismus der presse',\n",
    "           'abonnieren',\n",
    "           '(apa)',\n",
    "           'foto:',\n",
    "           'quelle:',\n",
    "           'lesezeit:',\n",
    "           'lesen sie mehr',\n",
    "           'stellenmarkt der sz.',\n",
    "           'bewerben sie sich jetzt',\n",
    "           'gutscheine',\n",
    "           'vergleichsportal',\n",
    "           'stern plus-inhalten',\n",
    "           'jederzeit kündbar',\n",
    "           'bereits registriert?',\n",
    "           'zur startseite',\n",
    "           'öffnet in neuem tab oder fenster',\n",
    "           'vollansicht der tabelle unter'        \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: del_patterns(str(x), pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Emojis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r'[^\\w\\s]', lambda match: map_emoji_to_description(match.group(), language = 'de',), str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1 = data_wo_pattern.copy()\n",
    "\n",
    "# strip_numeric -> removing digits (https://gensimr.news-r.org/reference/strip_numeric.html)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_numeric)\n",
    "\n",
    "# strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_punctuation)\n",
    "\n",
    "# strip multiple whitespaces also \\n -> (https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_multiple_whitespaces)\n",
    "\n",
    "#strip spanish accents\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: remove_accents(str(x)))\n",
    "\n",
    "#strip_short deletes words smaller 3\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_short)\n",
    " \n",
    "#strip links\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Create a list of stop words to remove \n",
    "stop_words_to_remove = ['nicht', 'nichts']\n",
    "\n",
    "stop_words_to_add = ['vor', 'in', 'den', 'dem', 'beim', 'wir', 'der', 'ist','ende', 'seite', 'ersten', 'fürs', 'eh', 'blick', 'schon', 'zumal', 'erst', 'mal', 'bild', 't', '(dpa)']\n",
    "\n",
    "# Remove the stop words to remove from the stop words list\n",
    "for word in stop_words_to_remove:\n",
    "  german_stop_words.remove(word)\n",
    "\n",
    "for word in stop_words_to_add:\n",
    "  german_stop_words.append(word)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'text' column using the apply method\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: remove_stopwords(x, german_stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer xabi alonso mitgereisten fans monaco familie genoss xabi alonso team hotel monaco moment erfolges dramatischen einzug achtelfinale europa league ,,aber gab zeit feiern\" sagte trainer bayer leverkusen personlich wohl wichtigste spiel jungen trainerlaufbahn erlebt alonso ,,es intensiv fur situation ruhig fokussiert besten entscheidungen treffen bessere emotionale kontrolle besser spiel leider konnten nicht verteidigen zufrieden leistung mannschaft gross anspannung elfmeterschiessen alonso ,,ehrlich vertrauen mittwoch training elfmeter geubt gut azmoun gut amiri gut klare idee spieler schiessen konnte bundesliga viele elfer verschossen funfmal folge getroffen effektiv passieren spieler gut gemacht gibt trick lotto gestern glucklichen glucksmoment exequiel palacios hebt tor euro highlight geht bundesliga sonntag uhr freiburg ,,wir kennen freiburg gerade guten moment spielen hohen intensitat ahnlich mainz vielleicht spiel mainz gute schule fur fur freiburg spiel\" sagt alonso bayer tross bleibt tag monaco samstag uhr fliegt team basel trainiert anschliessend freiburg wegen leverkusens strafstoss krise freche elfer aktion mainz torwart alonso ,,nach minuten vielen emotionen brauchen frische beine fur freiburg sehen spieler besten erholt stunde fur erholung wichtig periode brauchen ganze mannschaft ganzen kader brauchen spieler sonntag sicher wechseln personell klar verteidiger jonathan tah piero hincapie gelb gesperrt amine adli wegen rot sperre zuschauen europa league nicht gemeldete daley sinkgraven oberschenkel probleme uberwunden ,,er spielen\" erklart alonso moussa diaby besteht adduktoren problemen hoffnung kraft fur startelf reicht alonso ,,wir mussten monaco vorsichtig sehen wunsch immer spielen mussen sagen bleib\\' ruhig brauchen moussa besten form leverkusen siegt europa league dramatisch elferschiessen monaco bayer leverkusen gibt interessante massnahme fur auslandischen profis'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rm_1['data'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer xabi alonso mitgereisten fans monaco familie genoss xabi alonso team hotel monaco moment erfolges dramatischen einzug achtelfinale europa league ,,aber gab zeit feiern\" sagte trainer bayer leverkusen personlich wohl wichtigste spiel jungen trainerlaufbahn erlebt alonso ,,es intensiv fur situation ruhig fokussiert besten entscheidungen treffen bessere emotionale kontrolle besser spiel leider konnten nicht verteidigen zufrieden leistung mannschaft gross anspannung elfmeterschiessen alonso ,,ehrlich vertrauen mittwoch training elfmeter geubt gut azmoun gut amiri gut klare idee spieler schiessen konnte bundesliga viele elfer verschossen funfmal folge getroffen effektiv passieren spieler gut gemacht gibt trick lotto gestern glucklichen glucksmoment exequiel palacios hebt tor euro highlight geht bundesliga sonntag uhr freiburg ,,wir kennen freiburg gerade guten moment spielen hohen intensitat ahnlich mainz vielleicht spiel mainz gute schule fur fur freiburg spiel\" sagt alonso bayer tross bleibt tag monaco samstag uhr fliegt team basel trainiert anschliessend freiburg wegen leverkusens strafstoss krise freche elfer aktion mainz torwart alonso ,,nach minuten vielen emotionen brauchen frische beine fur freiburg sehen spieler besten erholt stunde fur erholung wichtig periode brauchen ganze mannschaft ganzen kader brauchen spieler sonntag sicher wechseln personell klar verteidiger jonathan tah piero hincapie gelb gesperrt amine adli wegen rot sperre zuschauen europa league nicht gemeldete daley sinkgraven oberschenkel probleme uberwunden ,,er spielen\" erklart alonso moussa diaby besteht adduktoren problemen hoffnung kraft fur startelf reicht alonso ,,wir mussten monaco vorsichtig sehen wunsch immer spielen mussen sagen bleib\\' ruhig brauchen moussa besten form leverkusen siegt europa league dramatisch elferschiessen monaco bayer leverkusen gibt interessante massnahme fur auslandischen profis'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rm_1['data'].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1 = name_wordgroups(data_rm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1.to_csv('/Users/kevingiesen/Library/Mobile Documents/com~apple~CloudDocs/BIPM Master/Semester 2/TWSM/TWSM Project/SS23-BIPM-Analytics-Lab---Group-4-repository/Preprocessing/data_clean/de_clean_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Transformation (setence with Playernames)\n",
    "The second transformation focus on the deletion of sentences to clean the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lines and following lines where the Player name appears in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into lower case\n",
    "data_lower = df_before.copy()\n",
    "\n",
    "data_lower['data'] = data_lower['data'].str.lower()\n",
    "data_lower['player'] = data_lower['player'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete content pattern and commercials\n",
    "data_lower['data'] = data_lower['data'].apply(lambda x: re.sub(r\"^{\\'content\\': \\'\", \"\", str(x)))\n",
    "data_lower['data'] = data_lower['data'].apply(lambda x: del_patterns(str(x), pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only paragraphs which include playernames \n",
    "data_with_playernames = find_lines_with_player(data_lower, data_lower['player'].unique(), n_lines = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform emojis into german text\n",
    "data_with_playernames['data'] = data_with_playernames['data'].apply(lambda x: re.sub(r'[^\\w\\s]', lambda match: map_emoji_to_description(match.group(), language = 'de',), str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_2 = data_with_playernames.copy()\n",
    "\n",
    "# strip_numeric -> removing digits (https://gensimr.news-r.org/reference/strip_numeric.html)\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_numeric)\n",
    "\n",
    "# strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_punctuation)\n",
    "\n",
    "# strip multiple whitespaces also \\n -> (https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_multiple_whitespaces)\n",
    "\n",
    "#strip spanish accents\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(lambda x: remove_accents(str(x)))\n",
    "\n",
    "#strip_short deletes words smaller 3\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_short)\n",
    " \n",
    "#strip links\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the remove_stopwords function to the 'text' column using the apply method\n",
    "#data_rm_2['data'] = data_rm_2['data'].apply(remove_stopwords_from_text)\n",
    "\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(lambda x: remove_stopwords(x, german_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer xabi alonso mitgereisten fans monaco glucksmoment exequiel palacios hebt tor euro highlight geht bundesliga sonntag uhr freiburg'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rm_2['data'].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wordgoups \n",
    "Matching first and last name to just last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_2 = name_wordgroups(data_rm_2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete empty rows and store csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_2.to_csv('/Users/kevingiesen/Library/Mobile Documents/com~apple~CloudDocs/BIPM Master/Semester 2/TWSM/TWSM Project/SS23-BIPM-Analytics-Lab---Group-4-repository/Preprocessing/data_clean/de_clean_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
