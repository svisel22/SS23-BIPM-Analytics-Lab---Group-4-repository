{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import remove_similar_rows_per_player, find_lines_with_player, del_patterns, map_emoji_to_description, remove_accents, name_wordgroups\n",
    "import emoji\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/data_files/all_data_v3.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Transformation (with Pattern deletion)\n",
    "The first transformation focus on the deletion of Patterns to clean the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out German data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the German data and reindex\n",
    "df_ger = df[df[\"language\"] == \"de\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove similiar rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similiar rows (The Function is imported from utils on top)\n",
    "df_ger = remove_similar_rows_per_player(df_ger, df_ger['player'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform Hincapie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform Hincapié to Hincapie\n",
    "df_ger.loc[df_ger['player'] == 'Piero Hincapié', 'player'] = 'Piero Hincapie'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into lower case\n",
    "data_lower = df_ger.copy()\n",
    "\n",
    "data_lower['data'] = data_lower['data'].str.lower()\n",
    "data_lower['player'] = data_lower['player'].str.lower()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete content patterns\n",
    "data_wo_pattern = data_lower.copy()\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r\"^{\\'content\\': \\'\", \"\", str(x)))\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r\"{'content':\", \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patterns to delete\n",
    "pattern = ['nutze kicker', \n",
    "           'mit dem', \n",
    "           'nur €2,49', \n",
    "           'https://www.faz.net/',\n",
    "           'bereits pur-abonnent?', \n",
    "           'alle antworten', \n",
    "           'hinweis zur verarbeitung', \n",
    "           'werbung','olympia-verlags', \n",
    "           'bild', 'g+j medien gmbh', \n",
    "           'services',\n",
    "           'kurz-link dieses artikels lautet:', \n",
    "           'http://epaper.welt.de',\n",
    "           'stephan von nocks',\n",
    "           'warum sehe ich faz.net',\n",
    "           'permalink:',\n",
    "           'mcfit mitgliedschaft',\n",
    "           'fitx-vertrag',\n",
    "           'kündigeneasyfitness',\n",
    "           'proteinbedarf',\n",
    "           'fitnessland',\n",
    "           'kündigeneasyfitness',\n",
    "           'trainingspuls berechnen',\n",
    "           'pulsrechner',\n",
    "           'fitseveneleven-mitgliedschaft',\n",
    "           'alkoholabbau & promille',\n",
    "           'index.promillerechner',\n",
    "           'mitgliedschaft kündigen',\n",
    "           'promillerechner',\n",
    "           'ihr body-mass-index',\n",
    "           'bmi rechner',\n",
    "           'kalorienrechner',\n",
    "           'grundumsatz & kalorienbedarf',\n",
    "           'partnerangebote',\n",
    "           'newsletter',\n",
    "           'journalismus der presse',\n",
    "           'abonnieren',\n",
    "           '(apa)',\n",
    "           'foto:',\n",
    "           'quelle:',\n",
    "           'lesezeit:',\n",
    "           'lesen sie mehr',\n",
    "           'stellenmarkt der sz.',\n",
    "           'bewerben sie sich jetzt',\n",
    "           'gutscheine',\n",
    "           'vergleichsportal',\n",
    "           'stern plus-inhalten',\n",
    "           'jederzeit kündbar',\n",
    "           'bereits registriert?',\n",
    "           'zur startseite',\n",
    "           'öffnet in neuem tab oder fenster',\n",
    "           'vollansicht der tabelle unter',\n",
    "           'frankfurter allgemeine zeitung'      \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete patterns\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: del_patterns(str(x), pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Emojis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r'[^\\w\\s]', lambda match: map_emoji_to_description(match.group(), language = 'de',), str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1 = data_wo_pattern.copy()\n",
    "\n",
    "# strip_numeric -> removing digits (https://gensimr.news-r.org/reference/strip_numeric.html)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_numeric)\n",
    "\n",
    "#strip links\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# strip multiple whitespaces also \\n -> (https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_multiple_whitespaces)\n",
    "\n",
    "#strip spanish accents\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: remove_accents(str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index \n",
    "data_rm_1 = data_rm_1.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty rows \n",
    "data_rm_1 = data_rm_1.replace('', pd.NA)\n",
    "data_rm_1.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store first Transformation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data \n",
    "data_rm_1.to_csv('/Users/kevingiesen/Library/Mobile Documents/com~apple~CloudDocs/BIPM Master/Semester 2/TWSM/TWSM Project/SS23-BIPM-Analytics-Lab---Group-4-repository/Preprocessing/data_clean/de_clean_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second transformation Preperation for sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data \n",
    "data_rm_2 = data_rm_1.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Create a list of stop words to remove \n",
    "stop_words_to_remove = ['nicht', 'nichts', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'anders']\n",
    "\n",
    "stop_words_to_add = ['vor', 'in', 'den', 'dem', 'beim', 'wir', 'der', 'ist','ende', 'seite', 'ersten', 'fürs', 'eh', 'blick', 'schon', 'zumal', 'erst', 'mal', 'bild', 't', '(dpa)', 'fur']\n",
    "\n",
    "# Remove the stop words to remove from the stop words list\n",
    "for word in stop_words_to_remove:\n",
    "  german_stop_words.remove(word)\n",
    "\n",
    "for word in stop_words_to_add:\n",
    "  german_stop_words.append(word)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'text' column using the apply method\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(lambda x: remove_stopwords(x, german_stop_words))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip punctuation and remove short words\n",
    "copy of unstriped data is taken because it's needed for further data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data for third transformation\n",
    "data_rm_3 = data_rm_2.copy()\n",
    "\n",
    "# strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_punctuation)\n",
    "\n",
    "#strip_short deletes words smaller 3\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_short)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store second transformation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_2.to_csv('/Users/kevingiesen/Library/Mobile Documents/com~apple~CloudDocs/BIPM Master/Semester 2/TWSM/TWSM Project/SS23-BIPM-Analytics-Lab---Group-4-repository/Preprocessing/data_clean/de_clean_2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Transformation (Paragraphs with Playernames)\n",
    "The third transformation focus on the deletion of sentences to clean the corpus. The only paragraphs kept are the one including the player names."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lines and following lines where the Player name appears in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only paragraphs which include playernames \n",
    "data_with_playernames = find_lines_with_player(data_rm_3, data_rm_3['player'].unique(), n_lines = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create wordgroups\n",
    "Remove first names of players and create word pairs e.g. for bayer leverkusen -> bayerleverkusen or europa league -> europaleague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform wordpair function\n",
    "data_with_playernames = name_wordgroups(data_with_playernames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete playernames from their sentences\n",
    "Because the playernames took a huge influence on the clustering they will be removed for each player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every player remove their names from the texts \n",
    "for player in data_with_playernames['player'].unique():\n",
    "    f_l_name = player.split()\n",
    "\n",
    "    # Extracting the first name\n",
    "    first_name = str(f_l_name[0])\n",
    "\n",
    "    # Extracting the last name\n",
    "    last_name = str(f_l_name[1])\n",
    "\n",
    "    updated_pattern = r\"\\b(\" + first_name.lower() + r\"|\" + last_name.lower() + r\")\\b|\"\n",
    "\n",
    "\n",
    "    # Apply the function to the data column\n",
    "    data_with_playernames.loc[data_with_playernames['player'] == player, 'data'] = data_with_playernames.loc[data_with_playernames['player'] == player, 'data'].apply(lambda x: re.sub(updated_pattern, \"\", str(x)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip punctuation and remove short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_with_playernames['data'] = data_with_playernames['data'].apply(strip_punctuation)\n",
    "\n",
    "#strip_short deletes words smaller 3\n",
    "data_with_playernames['data'] = data_with_playernames['data'].apply(strip_short)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty rows and store csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delet empty rows\n",
    "data_with_playernames = data_with_playernames.replace('', pd.NA)\n",
    "data_with_playernames.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store third transformation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_playernames.to_csv('/Users/kevingiesen/Library/Mobile Documents/com~apple~CloudDocs/BIPM Master/Semester 2/TWSM/TWSM Project/SS23-BIPM-Analytics-Lab---Group-4-repository/Preprocessing/data_clean/de_clean_condensed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
