{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Bert Multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook aims to process data that has been previously cleaned and provide a specific score for sentiment analysis. Once this score is generated, an accuracy analysis will be performed to check the performance of the applied model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_numeric, strip_punctuation, strip_multiple_whitespaces, strip_short\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used is bert-base-multilingual-uncased-sentiment from the hugging face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
    "\n",
    "# Creating the tokenizer for sentiment analysis using the specified pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Creating the model for sequence classification using the specified pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is load, previously cleaned in the preprocessing models. The csv that will be used is data_clean_1. This is a data set cleaned, which only contains the sentence where the player name is. It also contains 30 rows per language which has been manually labeled, to enable later accuracy performance report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of this specific dataset was based on two main factors. Firstly, it aligned with the specifications and examples that the BERT-multilingual model was designed to work with. Secondly, after testing various other cleaned datasets, this particular one demonstrated better accuracy and more reliable results. Some of the other datasets were yielding all positive or all negative results, making this dataset the preferred choice for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df_de = pd.read_csv('../Preprocessing/data_clean/labeled-data/labeled-de_clean_1-1.csv',sep = ';')\n",
    "#df_en = pd.read_csv('../Preprocessing/data_clean/en_clean_1.csv')\n",
    "df_en = pd.read_csv('../Preprocessing/data_clean/labeled-data/labeled-en_clean_1-1_not101010.csv')\n",
    "#df_es = pd.read_csv('../Preprocessing/data_clean/en_clean_1.csv')\n",
    "df_es = pd.read_csv('../Preprocessing/data_clean/labeled-data/labeled-es_clean_1-1.csv', sep = ';')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple function that allows the model to operate for each record in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function to assign a sentiment score for each record of the df\n",
    "def sentiment_score(data):\n",
    "    if isinstance(data, str):\n",
    "        tokens = tokenizer.encode(data, return_tensors='pt')\n",
    "        result = model(tokens)\n",
    "        return int(torch.argmax(result.logits)) + 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runing the funtion sentiment_score for each data set per language\n",
    "df_de['sentiment'] = df_de['data'].apply(lambda x: sentiment_score(x[:512]))\n",
    "df_en['sentiment'] = df_en['data'].apply(lambda x: sentiment_score(x[:512]))\n",
    "df_es['sentiment'] = df_es['data'].apply(lambda x: sentiment_score(x[:512]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate scores to positive, neutral and negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the results with the manually labled records, the following translation into positive, neutral and negative are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map for distribution\n",
    "sentiment_mapping = {1: 'negativ', 2: 'neutral', 3: 'neutral', 4: 'neutral', 5: 'positiv'}\n",
    "\n",
    "# Replace the numbers with labels using the mapping\n",
    "df_de['sentiment_label'] = df_de['sentiment'].map(sentiment_mapping)\n",
    "df_en['sentiment_label'] = df_en['sentiment'].map(sentiment_mapping)\n",
    "df_es['sentiment_label'] = df_es['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all sentiment scores have been added, the data needs to be stored for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes vertically\n",
    "merged_df = pd.concat([df_de, df_en, df_es], ignore_index=True)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "folder_name = 'data'\n",
    "\n",
    "# Define the file path for saving the CSV\n",
    "file_name = 'data_sentiment_final.csv'\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "# Convert the dataframe to CSV and save it\n",
    "merged_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the manually labeled rows an accuracy performance analysis is needed. The base for this analysis is the confusion matrix. The perofrmance per language per category, taking into acount f1, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_de.dropna(subset=['Label'], inplace=True)\n",
    "df_en.dropna(subset=['Label'], inplace=True)\n",
    "df_es.dropna(subset=['Label'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for exact match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy measure captures the exact match, indicating that the manually given label exactly matches the predicted-mapped label. In other words, it evaluates how many predicted labels are precisely the same as the ground-truth (manually assigned) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy DE: 56.67%\n",
      "Accuracy EN: 46.67%\n",
      "Accuracy ES: 43.33%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy_de = (df_de['sentiment_label'] == df_de['Label']).mean() * 100\n",
    "accuracy_en = (df_en['sentiment_label'] == df_en['Label']).mean() * 100\n",
    "accuracy_es = (df_es['sentiment_label'] == df_es['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy DE: {:.2f}%\".format(accuracy_de))\n",
    "print(\"Accuracy EN: {:.2f}%\".format(accuracy_en))\n",
    "print(\"Accuracy ES: {:.2f}%\".format(accuracy_es))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.56      0.50      0.53        10\n",
      "     neutral       0.53      0.80      0.64        10\n",
      "     positiv       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.59      0.57      0.56        30\n",
      "weighted avg       0.59      0.57      0.56        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting the actual sentiment labels\n",
    "actual_labels = df_de['Label']\n",
    "\n",
    "# Extracting the predicted sentiment labels\n",
    "predicted_labels = df_de['sentiment_label']\n",
    "\n",
    "# Calculating the classification report based on the actual and predicted sentiment labels\n",
    "report = classification_report(actual_labels, predicted_labels)\n",
    "\n",
    "# Displaying the classification report which includes precision, recall, F1-score\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.31      1.00      0.47         4\n",
      "     neutral       0.62      0.62      0.62        13\n",
      "     positiv       0.50      0.15      0.24        13\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.47      0.59      0.44        30\n",
      "weighted avg       0.52      0.47      0.43        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting the actual sentiment labels\n",
    "actual_labels = df_en['Label']\n",
    "\n",
    "# Extracting the predicted sentiment labels\n",
    "predicted_labels = df_en['sentiment_label']\n",
    "\n",
    "# Calculating the classification report based on the actual and predicted sentiment labels\n",
    "report = classification_report(actual_labels, predicted_labels)\n",
    "\n",
    "# Displaying the classification report which includes precision, recall, F1-score\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.42      0.50      0.45        10\n",
      "     neutral       0.44      0.80      0.57        10\n",
      "     positiv       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.29      0.43      0.34        30\n",
      "weighted avg       0.29      0.43      0.34        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galachaparro/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/galachaparro/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/galachaparro/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Extracting the actual sentiment labels\n",
    "actual_labels = df_es['Label']\n",
    "\n",
    "# Extracting the predicted sentiment labels\n",
    "predicted_labels = df_es['sentiment_label']\n",
    "\n",
    "# Calculating the classification report based on the actual and predicted sentiment labels\n",
    "report = classification_report(actual_labels, predicted_labels)\n",
    "\n",
    "# Displaying the classification report which includes precision, recall, F1-score\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.41      0.58      0.48        24\n",
      "     neutral       0.52      0.73      0.61        33\n",
      "     positiv       0.60      0.18      0.28        33\n",
      "\n",
      "    accuracy                           0.49        90\n",
      "   macro avg       0.51      0.50      0.46        90\n",
      "weighted avg       0.52      0.49      0.45        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concatenate the dataframes vertically\n",
    "merged_df = pd.concat([df_de, df_en, df_es], ignore_index=True)\n",
    "\n",
    "# Extracting the actual sentiment labels\n",
    "actual_labels = merged_df['Label']\n",
    "\n",
    "# Extracting the predicted sentiment labels\n",
    "predicted_labels = merged_df['sentiment_label']\n",
    "\n",
    "# Calculating the classification report based on the actual and predicted sentiment labels\n",
    "report = classification_report(actual_labels, predicted_labels)\n",
    "\n",
    "# Displaying the classification report which includes precision, recall, F1-score\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a CSV file that will contain the sentiment score and label predicted by the model for each row of data. Simultaneously, this provides us with information on the accuracy of the model for each language and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps for Buyer04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next steps for Buyer04, we would recommend exploring other models that are specifically designed for accurate analysis in the context of sports news. Different models might offer better performance and results for sentiment analysis in this domain.\n",
    "\n",
    "Another approach to improve accuracy is to find the most suitable way to map the sentiment scores into the labels positive, negative, and neutral. Fine-tuning this mapping process based on the specific characteristics of sports news data may lead to more precise sentiment predictions and better overall performance of the sentiment analysis system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
