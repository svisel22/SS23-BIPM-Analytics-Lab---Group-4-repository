{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and update articles per player per language with date using newsapi.org and Beautifulsoup\n",
    "\n",
    "## Intro\n",
    "The goal of this file is to get all articles from newsapi.org that include the given playername. Per article we store the information for which player it was retrieved and the information from the API of when the article was published and in which language. \n",
    "\n",
    "The free version of the API limits us in three ways: \n",
    "* we are only allowed to make 100 requests per day,\n",
    "* per request we will only get maximum 100 articles and\n",
    "* we will only be able to retrieve the data 1 month back.\n",
    "\n",
    "We nevertheless decided for this API, as the other APIs that we looked at even had more restrictions.\n",
    "\n",
    "Therefore we created an initial csv with the following steps: \n",
    "1. Get data for one player for one language:                get_articlesplayer\n",
    "2. Get data for multiple players and multiple languages:    get_players_df        using get_articlesplayer\n",
    "And then we updated this csv, at least every month, with the following steps:\n",
    "3. Get the Date where a file was last updated:              get_date_of_file\n",
    "4. Update a given file:                                     updateplayerdata      using get_players_df\n",
    "Duplicates will be filtered out in the Preprocessing ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "# API Key\n",
    "secret = '2211c202d86d46f78b48b3e532557d7d'\n",
    "# Define the endpoint\n",
    "url = 'https://newsapi.org/v2/everything'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function get_articlesplayer \n",
    "The function takes a player, language and a date from and until which the article data should be created, as an input. \n",
    "The function then calls the api and stores all article urls in a list called urls and all dates in a list called dates.\n",
    "BeautifulSoup is then used to parse each HTML response per url in urls and concatenates all paragraphs into one string. The resulting strings are stored in a dataframe called articles. \n",
    "The function returns this dataframe and the dates list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articlesplayer(player, language, date_to):\n",
    "\n",
    "    # Set the url from newsapi\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "    # Create date_form which is the current date\n",
    "    today = datetime.date.today()\n",
    "    date_from = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Specify the query and number of returns\n",
    "    parameters = {\n",
    "        'q': player, # query phrase\n",
    "        'pageSize': 100,  # maximum is 100\n",
    "        'apiKey': secret, # your own API key\n",
    "        'sortBy':'publishedAt',\n",
    "        'from': date_from,\n",
    "        'to': date_to, \n",
    "        'language': language\n",
    "    }\n",
    "\n",
    "    # get the response\n",
    "    response = requests.get(url, params = parameters)\n",
    "\n",
    "    # Parse the JSON response and extract article URLs\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        urls = [article['url'] for article in data['articles']]\n",
    "        dates = [article['publishedAt'] for article in data['articles']]\n",
    "\n",
    "    else:\n",
    "        print('Error: Request failed with status', response.status_code)\n",
    "\n",
    "    # Scrape the full article content for each URL and store in a dataframe\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        # Make a request to the article URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parse the HTML response and extract the article content\n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "            content = '\\n'.join([p.text.strip() for p in soup.find_all('p')])\n",
    "            articles.append({'content': content})\n",
    "        else:\n",
    "            articles.append('-1')\n",
    "            print('Error: Request failed with status', response.status_code)\n",
    "\n",
    "\n",
    "\n",
    "    return(articles, dates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get articles for multiple players and multiple Languages\n",
    "The get_players_df takes a list of players, a list of languages and a date until the articles should be collected as an input. \n",
    "A dataframe df_all_players will be created with columns for the articles content, playernames, the given language and the date the article was published. \n",
    "The function then loops over the list of players, and within that over the list of langauges, and calls the get_articlesplayer function per language and per player. Each time the articles content, playernames, the given language and the date the article was published is stored in df_one_lang and then appended to df_all_players.\n",
    "The function returns the df_all_players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_players_df(playerlist, languagelist, date_to):\n",
    "\n",
    "    # Create an empty Datframe where all player data is stored\n",
    "    df_all_players = pd.DataFrame(columns=['data', 'player', 'language','publishedAt'])\n",
    "\n",
    "    # Loop over all players in the player list \n",
    "    for player in playerlist:\n",
    "\n",
    "    # Loop over all languages in the languages list \n",
    "        for language in languagelist:\n",
    "\n",
    "            # Call the get_articlesplayer to get the articles for a given player and language\n",
    "            articles_arr, dates_arr = get_articlesplayer(player, language, date_to)\n",
    "\n",
    "            # Create an array with the current language of the same size as the articles data\n",
    "            language_arr = np.full((len(articles_arr)), language)\n",
    "\n",
    "            # Create an array with the current player name of the same size as the articles data\n",
    "            player_arr = np.full((len(articles_arr)), player)\n",
    "\n",
    "            # Create a datframe with the articles, playername and language\n",
    "            df_one_lang = pd.DataFrame({'data': articles_arr, 'player': player_arr,'language': language_arr, 'publishedAt': dates_arr})\n",
    "\n",
    "            # Add the data of the iteration to the prior data \n",
    "            df_all_players = pd.concat([df_all_players, df_one_lang])\n",
    "        \n",
    "        df_all_players = df_all_players[df_all_players[\"data\"] != \"-1\"]\n",
    "        \n",
    "    return df_all_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save initial dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the list of players which should be selected\n",
    "# playerlist = ['Mitchel Bakker', 'Jeremie Frimpong', 'Moussa Diaby', 'Jonathan Tah', 'Piero Hincapie', 'Piero Hincapié', 'Exequiel Palacios', 'Mykhaylo Mudryk'] \n",
    "\n",
    "# # Create a list of languages \n",
    "# languagelist = ['en', 'de', 'es']\n",
    "\n",
    "# # Create the date_to\n",
    "# today = datetime.date.today()\n",
    "# thirty_days_ago = today - datetime.timedelta(days=30)\n",
    "# date_to = thirty_days_ago.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# # Apply the function get_players_df\n",
    "# df_all_players = get_players_df(playerlist, languagelist, date_to)\n",
    "\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = \"../data_files\"\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = os.path.join(folder_path, \"all_data.csv\")\n",
    "\n",
    "# # Save the DataFrame as a CSV file\n",
    "# df_all_players.to_csv(file_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function get_date_of_file\n",
    "The function takes a file as an input and delivers the last updated time of the file as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_of_file(file):\n",
    "    # get the last time the file was updated\n",
    "    creation_time = os.stat(file)\n",
    "    last_modified_time = creation_time.st_mtime\n",
    "\n",
    "    # convert the float time into readable format\n",
    "    seconds = int(last_modified_time)\n",
    "    microseconds = int((last_modified_time - seconds) * 1000000)\n",
    "    dta = dt.fromtimestamp(seconds).replace(microsecond=microseconds)\n",
    "    date_string = dta.strftime('%m-%d-%Y')\n",
    "\n",
    "    return date_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function updateplayerdata\n",
    "Within the function a file which needs to be updated is taken as an input. \n",
    "The function collects all player data from the last day of updating and then appends the data to the existing csv in append mode.\n",
    "The output id df_new that now includes the data that was in the csv before aswell as the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateplayerdata(file):\n",
    "    # Create the list of players which should be selected\n",
    "    playerlist = ['Mitchel Bakker', 'Jeremie Frimpong', 'Moussa Diaby', 'Jonathan Tah', 'Piero Hincapie', 'Piero Hincapié', 'Exequiel Palacios', 'Mykhaylo Mudryk'] \n",
    "    # Create a list of languages \n",
    "    languagelist = ['en', 'de', 'es']\n",
    "\n",
    "    # Call the get_players_df function on the players, languages and the last time the file was updated \n",
    "    df_new = get_players_df(playerlist, languagelist, get_date_of_file(file))\n",
    "\n",
    "    # Open the CSV file in append mode and write the DataFrame\n",
    "    with open(file, 'a', newline='') as f:\n",
    "        df_new.to_csv(f, header=f.tell()==0, index=False)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update existing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '../data_files/all_data_v3.csv'\n",
    "\n",
    "# updateplayerdata(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This file is used to get and update the data that we gather from the website's urls which we gather from the newsapi.org API. \n",
    "The output is the csv all_data_v3.csv which is stored in the data folder.\n",
    "\n",
    "# Next steps for Bayer04 Leverkusen\n",
    "To further improve the data gathering Bayer04 could change to the paid version of the API, which would allow them to gather more data, more recent (real-time) data and data which is up to 5 years old. For further information: https://newsapi.org/pricing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
