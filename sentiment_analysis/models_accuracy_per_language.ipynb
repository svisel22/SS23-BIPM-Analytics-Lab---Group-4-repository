{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\joana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries adn load dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models:\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification # for german\n",
    "from pattern.en import sentiment # for english\n",
    "import nltk # for english\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for english\n",
    "nltk.download('vader_lexicon') # for english\n",
    "from pysentimiento import create_analyzer # for spanish\n",
    "\n",
    "# Accuracy\n",
    "from utils import sentiment_translated_scores, evaluate_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More sentiment models and their accuracy per language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "TODO\n",
    "\n",
    "full text\n",
    "extracted sentence\n",
    "\n",
    "full condensed\n",
    "extracted sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labeled data: d1.1\n",
    "Explanation why we used d1.1 and that is already with extracted sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled CSV files into a DataFrame\n",
    "df_de_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-de_clean_1-1.csv', sep=';')\n",
    "df_en_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-en_clean_1-1_not101010.csv')\n",
    "df_es_1 = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-es_clean_1-1.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labeled data: data condensed for english\n",
    "Explanation why we used data condensed for english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled CSV file into a DataFrame\n",
    "df_en_con = pd.read_csv('https://raw.githubusercontent.com/svisel22/SS23-BIPM-Analytics-Lab---Group-4-repository/main/Preprocessing/data_clean/labeled-data/labeled-en_clean_con_sen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German\n",
    "For the german data we used one additional model to the multilingual only, because then we focused on clustering.\n",
    "\n",
    "### Model: AutoTokenizer and AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "The model used was trained on 1.834 million German-language samples sepcifically for sentiment classification.\n",
    "https://huggingface.co/oliverguhr/german-sentiment-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  data    player language  \\\n",
      "14   zweimal verwandelte palacios, es wurde ein ube...  palacios       de   \n",
      "23   schalke: reis nimmt den spieler ausdrucklich i...  palacios       de   \n",
      "31   palacios (r.) verletzte sich leicht im hinspie...  palacios       de   \n",
      "41   \"auf jeden fall\" sei das ein ganz grosser tag,...  palacios       de   \n",
      "58   beim abschlusstraining trug letzterer zumindes...  palacios       de   \n",
      "63   bayerleverkusen argentinischer weltmeister pal...  palacios       de   \n",
      "83   \"es gibt im moment nicht viel besseres in der ...  frimpong       de   \n",
      "93   der fc bayern munchen interessiert sich nach i...  frimpong       de   \n",
      "95   frimpong hat noch bis vertrag in bayerleverkus...  frimpong       de   \n",
      "102  hincapie (l.) und tah bedanken sich fur die fa...  frimpong       de   \n",
      "120  diaby, wirtz, adam hlozek und frimpong teilten...  frimpong       de   \n",
      "123     bayerleverkusen torschutze amiri (rechts) u...  frimpong       de   \n",
      "124  frimpong bekommt gluckwunsche seines trainers ...  frimpong       de   \n",
      "130  selbst als stammkraft frimpong in den vergange...  frimpong       de   \n",
      "163  ohne hincapie und tah mussten wir entscheiden,...       tah       de   \n",
      "173  schalke wiederum hatte durch michael frey ein ...       tah       de   \n",
      "180   bayerleverkusen-coach alonso rotierte im verg...       tah       de   \n",
      "199  alonso \"stolz\": werkself seit spielen ungeschl...       tah       de   \n",
      "214  torwart lukas hradecky bewahrte bayerleverkuse...       tah       de   \n",
      "223  zudem drohen sowohl bakker\\xaals auch weltmeis...    bakker       de   \n",
      "258  torschutzen unter sich: diaby (links, treffer ...    bakker       de   \n",
      "273               beim : hatte mitchell bakker gepatzt    bakker       de   \n",
      "334                        minute uber wirtz und diaby     diaby       de   \n",
      "355  das team wird zudem von den beiden bayerleverk...     diaby       de   \n",
      "361  frimpong wirkt uberspielt, und diaby bewegte s...     diaby       de   \n",
      "362  torwart lukas hradecky bewahrte bayerleverkuse...     diaby       de   \n",
      "383  zu sehen ist davon wenig - und das liegt nicht...    mudryk       de   \n",
      "413  hincapie von fussball-bundesligist bayerleverk...  hincapie       de   \n",
      "433  bayerleverkusen torwart lukas hradecky sitzt f...  hincapie       de   \n",
      "436  in der funften minute der nachspielzeit sah ba...  hincapie       de   \n",
      "\n",
      "              publishedAt    Label sentiment_bert  \n",
      "14   2023-03-19T20:01:45Z  positiv        negativ  \n",
      "23   2023-04-03T08:12:20Z  neutral        negativ  \n",
      "31   2023-04-17T06:50:19Z  neutral        negativ  \n",
      "41   2023-04-20T20:54:03Z  positiv        negativ  \n",
      "58   2023-05-17T11:20:50Z  neutral        negativ  \n",
      "63   2023-05-14T18:07:21Z  negativ        negativ  \n",
      "83   2023-03-31T07:59:57Z  positiv        negativ  \n",
      "93   2023-04-06T16:49:00Z  neutral        negativ  \n",
      "95   2023-04-07T07:42:27Z  neutral        negativ  \n",
      "102  2023-04-14T08:16:22Z  positiv        negativ  \n",
      "120  2023-04-21T10:02:05Z  neutral        negativ  \n",
      "123  2023-04-23T17:33:57Z  positiv        negativ  \n",
      "124  2023-04-23T18:00:31Z  positiv        negativ  \n",
      "130  2023-05-23T10:24:42Z  negativ        negativ  \n",
      "163  2023-02-27T15:54:01Z  neutral       positive  \n",
      "173  2023-04-01T15:44:09Z  positiv        negativ  \n",
      "180  2023-04-16T19:29:40Z  neutral        negativ  \n",
      "199  2023-04-23T20:48:00Z  positiv        negativ  \n",
      "214  2023-05-11T23:42:49Z  positiv        negativ  \n",
      "223  2023-02-23T09:06:07Z  negativ        negativ  \n",
      "258  2023-04-20T21:15:24Z  positiv        negativ  \n",
      "273  2023-05-21T19:39:14Z  negativ        negativ  \n",
      "334  2023-04-29T15:37:30Z  neutral       positive  \n",
      "355  2023-05-13T10:26:28Z  neutral        negativ  \n",
      "361  2023-05-12T09:44:10Z  negativ        negativ  \n",
      "362  2023-05-11T23:42:49Z  negativ        negativ  \n",
      "383  2023-04-19T06:10:46Z  negativ        negativ  \n",
      "413  2023-05-23T10:52:44Z  negativ        negativ  \n",
      "433  2023-02-17T17:14:46Z  negativ        negativ  \n",
      "436  2023-05-21T19:53:27Z  negativ        negativ  \n"
     ]
    }
   ],
   "source": [
    "def german_sentiment_model(df):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "    # Create an empty list to store the sentiment scores\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # Iterate over the 'data' column in the DataFrame\n",
    "    for text in df['data']:\n",
    "        # Tokenize the input text\n",
    "        tokens = tokenizer.encode_plus(text, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                       return_tensors=\"pt\")\n",
    "\n",
    "        # Perform the sentiment analysis\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokens)[0]\n",
    "\n",
    "        # Convert logits to predicted label (positive/negative)\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        sentiment = \"positive\" if predicted_label == 1 else \"negativ\"\n",
    "\n",
    "        # Append the sentiment score to the list\n",
    "        sentiment_scores.append(sentiment)\n",
    "\n",
    "    # Add the sentiment scores as a new column in the DataFrame\n",
    "    df['sentiment_bert'] = sentiment_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Perform sentiment analysis on the DataFrame\n",
    "df_de_1 = german_sentiment_model(df_de_1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df_de_1)\n",
    "\n",
    "\n",
    "\n",
    "#ACTION: NOTE FOR JOANA: check whether this makes a new dataframe instead of appending a column. If so: Change & check what "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTION: Show distribution? If so: Do for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for german bert model\n",
    "TODO and todo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_de_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_de_1_pos_neg = df_de_1[df_de_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for oliverguhr/german-sentiment-bert\n",
      "Accuracy:  0.5  Unique labels:  ['negativ']\n",
      "Confusion matrix: \n",
      "         negativ  positiv\n",
      "negativ       10        0\n",
      "positiv       10        0\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negativ       0.50      1.00      0.67        10\n",
      "     positiv       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.25      0.50      0.33        20\n",
      "weighted avg       0.25      0.50      0.33        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for oliverguhr/german-sentiment-bert')\n",
    "accuracy_de, unique_predicted_de, confusion_matrix_de, classification_report_de = evaluate_performance(df_de_1_pos_neg, 'sentiment_bert', 'Label')\n",
    "\n",
    "print('Accuracy: ', accuracy_de,' Unique labels: ', unique_predicted_de)\n",
    "\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_de)\n",
    "print('Classification report: ')\n",
    "print(classification_report_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English - d1.1\n",
    "TODO\n",
    "\n",
    "### Why we used the pipeline for the english and spanish models\n",
    "TODO\n",
    "\n",
    "\n",
    "### Model 1: sentiment-analysis from bert-base-uncased\n",
    "TODO\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "sentiment_classifier_en = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Perform model and translate scores into two- and threedimensional labels\n",
    "sentiment_2_scores, sentiment_2_labels, sentiment_3_labels = sentiment_translated_scores(df_en_1, sentiment_classifier_en)\n",
    "\n",
    "# Create columns and assign score and labels to the columns\n",
    "df_en_1['sentiment_2_score_bert'] = sentiment_2_scores\n",
    "df_en_1['sentiment_2_label_bert'] = sentiment_2_labels\n",
    "df_en_1['sentiment_3_label_bert'] = sentiment_3_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m sentiment_2_labels, sentiment_3_labels\n\u001b[0;32m     20\u001b[0m \u001b[39m# Perform model and translate scores into two- and threedimensional labels\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m sentiment_2_scores, sentiment_2_labels, sentiment_3_labels \u001b[39m=\u001b[39m sentiment_translate_scores(df_en_1, \u001b[39m'\u001b[39;49m\u001b[39msentiment_2_score_bert\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m df_en_1\n",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m, in \u001b[0;36msentiment_translate_scores\u001b[1;34m(df, sentiment_score)\u001b[0m\n\u001b[0;32m      7\u001b[0m sentiment_2_labels\u001b[39m.\u001b[39mappend(sentiment_2_label)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Convert sentiment score to three-dimensional label (positive/neutral/negative)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mif\u001b[39;00m sentiment_score \u001b[39m>\u001b[39;49m \u001b[39m0.2\u001b[39;49m:\n\u001b[0;32m     11\u001b[0m     sentiment_3_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpositiv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39melif\u001b[39;00m sentiment_score \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.2\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Model imported as sentiment\n",
    "def sentiment_translate_scores(df, sentiment_score):\n",
    "    # Iterate over the score column in the DataFrame\n",
    "    for score in df[sentiment_score]:        \n",
    "        # Convert sentiment score to two-dimensional label (positive/negative)\n",
    "        sentiment_2_label = \"positiv\" if score > 0 else \"negativ\"\n",
    "        sentiment_2_labels.append(sentiment_2_label)\n",
    "\n",
    "        # Convert sentiment score to three-dimensional label (positive/neutral/negative)\n",
    "        if sentiment_score > 0.2:\n",
    "            sentiment_3_label = \"positiv\"\n",
    "        elif sentiment_score < -0.2:\n",
    "            sentiment_3_label = \"negativ\"\n",
    "        else:\n",
    "            sentiment_3_label = \"neutral\"\n",
    "        sentiment_3_labels.append(sentiment_3_label)\n",
    "\n",
    "    return sentiment_2_labels, sentiment_3_labels\n",
    "\n",
    "# Perform model and translate scores into two- and threedimensional labels\n",
    "sentiment_2_scores, sentiment_2_labels, sentiment_3_labels = sentiment_translate_scores(df_en_1, 'sentiment_2_score_bert')\n",
    "\n",
    "df_en_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: sentiment form pattern.en\n",
    "TODO\n",
    "\n",
    "https://stackabuse.com/python-for-nlp-introduction-to-the-pattern-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>player</th>\n",
       "      <th>language</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentiment_bert</th>\n",
       "      <th>sentiment_pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten if you included the toe-poked volley to te...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-16T23:56:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayerleverkusen took the lead again in the st ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:50:50Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705472</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wissam ben yedder levelled straight away from ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:53:59Z</td>\n",
       "      <td>positiv</td>\n",
       "      <td>0.702478</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midfielders: leandro paredes (juventus), angel...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T16:40:46Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.757339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>midfielders: rodrigo de paul (atletico madrid)...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T18:17:37Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.754591</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data    player language  \\\n",
       "0  ten if you included the toe-poked volley to te...  palacios       en   \n",
       "1  bayerleverkusen took the lead again in the st ...  palacios       en   \n",
       "2  wissam ben yedder levelled straight away from ...  palacios       en   \n",
       "3  midfielders: leandro paredes (juventus), angel...  palacios       en   \n",
       "4  midfielders: rodrigo de paul (atletico madrid)...  palacios       en   \n",
       "\n",
       "            publishedAt    Label  sentiment_bert  sentiment_pattern  \n",
       "0  2023-02-16T23:56:00Z      NaN        0.640392           0.000000  \n",
       "1  2023-02-23T20:50:50Z      NaN        0.705472           0.208333  \n",
       "2  2023-02-23T20:53:59Z  positiv        0.702478           0.100000  \n",
       "3  2023-03-03T16:40:46Z  neutral        0.757339           0.000000  \n",
       "4  2023-03-03T18:17:37Z  neutral        0.754591           0.200000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sentiment(text)[0]\n",
    "    return sentiment_score\n",
    "\n",
    "# Apply sentiment analysis to the \"data\" column and store the sentiment in a new column \"sentiment_pattern\"\n",
    "df_en_1['sentiment_pattern'] = df_en_1['data'].apply(sentiment)\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Sentiment Intensity Analyzer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>player</th>\n",
       "      <th>language</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentiment_bert</th>\n",
       "      <th>sentiment_pattern</th>\n",
       "      <th>sentiment_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten if you included the toe-poked volley to te...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-16T23:56:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayerleverkusen took the lead again in the st ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:50:50Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705472</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>-0.0516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wissam ben yedder levelled straight away from ...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-02-23T20:53:59Z</td>\n",
       "      <td>positiv</td>\n",
       "      <td>0.702478</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midfielders: leandro paredes (juventus), angel...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T16:40:46Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.757339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>midfielders: rodrigo de paul (atletico madrid)...</td>\n",
       "      <td>palacios</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-03-03T18:17:37Z</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.754591</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data    player language  \\\n",
       "0  ten if you included the toe-poked volley to te...  palacios       en   \n",
       "1  bayerleverkusen took the lead again in the st ...  palacios       en   \n",
       "2  wissam ben yedder levelled straight away from ...  palacios       en   \n",
       "3  midfielders: leandro paredes (juventus), angel...  palacios       en   \n",
       "4  midfielders: rodrigo de paul (atletico madrid)...  palacios       en   \n",
       "\n",
       "            publishedAt    Label  sentiment_bert  sentiment_pattern  \\\n",
       "0  2023-02-16T23:56:00Z      NaN        0.640392           0.000000   \n",
       "1  2023-02-23T20:50:50Z      NaN        0.705472           0.208333   \n",
       "2  2023-02-23T20:53:59Z  positiv        0.702478           0.100000   \n",
       "3  2023-03-03T16:40:46Z  neutral        0.757339           0.000000   \n",
       "4  2023-03-03T18:17:37Z  neutral        0.754591           0.200000   \n",
       "\n",
       "   sentiment_nltk  \n",
       "0          0.0000  \n",
       "1         -0.0516  \n",
       "2          0.2263  \n",
       "3          0.0000  \n",
       "4          0.0000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "\n",
    "# Apply sentiment analysis to the \"data\" column and store the sentiment in a new column \"sentiment_nltk\"\n",
    "df_en_1['sentiment_nltk'] = df_en_1['data'].apply(get_sentiment)\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for all english models\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_en_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en_1_pos_neg = df_en_1[df_en_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for bert-base-uncased\n",
      "Accuracy for bert: 0.00%\n",
      "Unique labels for bert:\n",
      "[0.70247829 0.71411204 0.65856969 0.70102882 0.71734881 0.70871013\n",
      " 0.60306615 0.7403965  0.71762341 0.68655038 0.68715137 0.69438171\n",
      " 0.72599542 0.6514957  0.6753307  0.71295387 0.64372659]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m predicted_labels \u001b[39m=\u001b[39m df_en_1_pos_neg[\u001b[39m'\u001b[39m\u001b[39msentiment_bert\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[39m# Create the confusion matrix\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Convert the confusion matrix to a DataFrame for better visualization\u001b[39;00m\n\u001b[0;32m     24\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate((true_labels, predicted_labels)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     92\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     97\u001b[0m             type_true, type_pred\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for bert-base-uncased')\n",
    "\n",
    "accuracy_en_bert, unique_predicted_en_bert, confusion_matrix_en_bert, classification_report_en_bert = evaluate_performance(df_en_1_pos_neg, 'sentiment_bert', 'Label')\n",
    "\n",
    "print('Accuracy: ', accuracy_en_bert,' Unique labels: ', unique_predicted_en_bert)\n",
    "\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_en_bert)\n",
    "print('Classification report: ')\n",
    "print(classification_report_en_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for pattern.en\n",
      "Accuracy for pattern.en: 0.00%\n",
      "Unique labels for pattern.en:\n",
      "[ 0.1         0.29444444  0.5         0.06666667  0.          0.05555556\n",
      " -0.6         0.31666667  0.07142857  0.4        -0.4         0.24833333]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m predicted_labels \u001b[39m=\u001b[39m df_en_1_pos_neg[\u001b[39m'\u001b[39m\u001b[39msentiment_pattern\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[39m# Create the confusion matrix\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Convert the confusion matrix to a DataFrame for better visualization\u001b[39;00m\n\u001b[0;32m     24\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate((true_labels, predicted_labels)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     92\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     97\u001b[0m             type_true, type_pred\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for pattern.en')\n",
    "\n",
    "#ADAPT\n",
    "accuracy_en_bert, unique_predicted_en_bert, confusion_matrix_en_bert, classification_report_en_bert = evaluate_performance(df_en_1_pos_neg, 'sentiment_bert', 'Label')\n",
    "\n",
    "print('Accuracy: ', accuracy_en_bert,' Unique labels: ', unique_predicted_en_bert)\n",
    "\n",
    "print('Confusion matrix: ')\n",
    "print(confusion_matrix_en_bert)\n",
    "print('Classification report: ')\n",
    "print(classification_report_en_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance evaluation for nltk\n",
      "Accuracy for nltk: 0.00%\n",
      "Unique labels for nltk:\n",
      "[ 0.2263  0.5859  0.1531  0.8625  0.      0.3818 -0.4939  0.5106 -0.7096\n",
      "  0.5975 -0.3612  0.8519]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m predicted_labels \u001b[39m=\u001b[39m df_en_1_pos_neg[\u001b[39m'\u001b[39m\u001b[39msentiment_nltk\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[39m# Create the confusion matrix\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Convert the confusion matrix to a DataFrame for better visualization\u001b[39;00m\n\u001b[0;32m     24\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate((true_labels, predicted_labels)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     92\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     97\u001b[0m             type_true, type_pred\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print('Performance evaluation for nltk')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_en_nltk = (df_en_1_pos_neg['sentiment_nltk'] == df_en_1_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for nltk: {:.2f}%\".format(accuracy_en_nltk))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for nltk:')\n",
    "print(df_en_1_pos_neg['sentiment_nltk'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_en_1_pos_neg['Label']\n",
    "predicted_labels = df_en_1_pos_neg['sentiment_nltk']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_en_nltk = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for nltk:\")\n",
    "print(cm_df_en_nltk)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_en_nltk = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for nltk:\")\n",
    "print(report_en_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish\n",
    "TODO?\n",
    "\n",
    "### Model 1: sentiment-analysis from spanish bert: beto-sentiment-analysis\n",
    "TODO\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis\n",
    "Although this model was trained on tweets, not on news data, it is a popular model to use for sentiment analysis of spanish data.\n",
    "The base model is BETO, a BERT model trained on spanish data (for further information: https://github.com/dccuchile/beto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier_es_beto = pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_beto\"\n",
    "df_es_1['sentiment_beto'] = df_es_1['data'].apply(lambda x: sentiment_classifier_es_beto(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_es_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: sentiment-analysis from another spanish bert: bert-base-spanish-wwm-uncased\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier_es_bert = pipeline('sentiment-analysis', model='dccuchile/bert-base-spanish-wwm-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_bert\"\n",
    "df_es_1['sentiment_bert'] = df_es_1['data'].apply(lambda x: sentiment_classifier_es_bert(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_es_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for all spanish models\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_es_1.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_es_1_pos_neg = df_es_1[df_es_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for beto-sentiment-analysis')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_es_beto = (df_es_1_pos_neg['sentiment_beto'] == df_es_1_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for beto: {:.2f}%\".format(accuracy_es_beto))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for beto:')\n",
    "print(df_es_1_pos_neg['sentiment_beto'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_es_1_pos_neg['Label']\n",
    "predicted_labels = df_es_1_pos_neg['sentiment_beto']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_es_beto = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for beto:\")\n",
    "print(cm_df_es_beto)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_es_beto = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for beto:\")\n",
    "print(report_es_beto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for bert-base-spanish-wwm-uncased')\n",
    "\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_es_bert = (df_es_1_pos_neg['sentiment_bert'] == df_es_1_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for bert: {:.2f}%\".format(accuracy_es_bert))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for bert:')\n",
    "print(df_es_1_pos_neg['sentiment_bert'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_es_1_pos_neg['Label']\n",
    "predicted_labels = df_es_1_pos_neg['sentiment_bert']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_es_bert = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for bert:\")\n",
    "print(cm_df_es_bert)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_es_bert = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for beto:\")\n",
    "print(report_es_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English: data condensed\n",
    "TODO\n",
    "\n",
    "### Model 1: sentiment-analysis from bert-base-uncased\n",
    "TODO\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loaded above: sentiment_classifier_en = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Apply sentiment analysis on the 'data' column  and store the sentiment in a new column \"sentiment_bert\"\n",
    "df_en_con['sentiment_bert'] = df_en_con['data'].apply(lambda x: sentiment_classifier_en(x)[0]['score'])\n",
    "\n",
    "# Print the updated dataframe\n",
    "df_en_con.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for english condensed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Label' is NaN or empty\n",
    "df_en_con.dropna(subset=['Label'], inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_en_con_pos_neg = df_en_con[df_en_1['Label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance evaluation for bert-base-uncased on english condensed')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_en_con_bert = (df_en_con_pos_neg['sentiment_bert'] == df_en_con_pos_neg['Label']).mean() * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for bert: {:.2f}%\".format(accuracy_en_con_bert))\n",
    "\n",
    "\n",
    "\n",
    "# Print unique predicted sentiment labels\n",
    "print('Unique labels for bert:')\n",
    "print(df_en_con_pos_neg['sentiment_bert'].unique())\n",
    "\n",
    "\n",
    "# # Assign true and predicted labels\n",
    "true_labels = df_en_con_pos_neg['Label']\n",
    "predicted_labels = df_en_con_pos_neg['sentiment_bert']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "cm_df_en_con_bert = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for bert:\")\n",
    "print(cm_df_en_con_bert)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the classification report\n",
    "report_en_con_bert = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report for bert:\")\n",
    "print(report_en_con_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
