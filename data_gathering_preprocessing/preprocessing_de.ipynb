{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing German Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, specifically for the German data, we will apply a series of techniques to remove ads, noise, and any information that is not needed for our analysis. We will clean the data that has been previously extracted and store it in different files and structures to fulfill the requirements of the models that will be applied later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import remove_similar_rows_per_player, find_lines_with_player, del_patterns, map_emoji_to_description, remove_accents, name_wordgroups, extract_sentence\n",
    "import emoji\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from our csv file with all the previously pulled data. And we filter German language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '../data_files/all_data_v3.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the German data and reindex\n",
    "df_ger = df[df[\"language\"] == \"de\"]\n",
    "\n",
    "#Reset index\n",
    "df_ger = df_ger.reset_index(drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove similiar rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a pre-tailored function to remove duplicates and rows that were mistakenly stored twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similiar rows (The Function is imported from utils on top)\n",
    "df_ger = remove_similar_rows_per_player(df_ger, df_ger['player'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform Hincapie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform Hincapié to Hincapie\n",
    "df_ger.loc[df_ger['player'] == 'Piero Hincapié', 'player'] = 'Piero Hincapie'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data into lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform data and player into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy\n",
    "data_lower = df_ger.copy()\n",
    "\n",
    "# Transform data into lower case\n",
    "data_lower['data'] = data_lower['data'].str.lower()\n",
    "data_lower['player'] = data_lower['player'].str.lower()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large number of noice and irrelevant information, patterns are defined to be removed from all texts. These patterns are specific for German texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy and delete content patterns\n",
    "data_wo_pattern = data_lower.copy()\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r\"^{\\'content\\': \\'\", \"\", str(x)))\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r\"{'content':\", \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns to delete\n",
    "pattern = ['nutze kicker', \n",
    "           'mit dem', \n",
    "           'nur €2,49', \n",
    "           'https://www.faz.net/',\n",
    "           'bereits pur-abonnent?', \n",
    "           'alle antworten', \n",
    "           'hinweis zur verarbeitung', \n",
    "           'werbung','olympia-verlags', \n",
    "           'bild', 'g+j medien gmbh', \n",
    "           'services',\n",
    "           'kurz-link dieses artikels lautet:', \n",
    "           'http://epaper.welt.de',\n",
    "           'stephan von nocks',\n",
    "           'warum sehe ich faz.net',\n",
    "           'permalink:',\n",
    "           'mcfit mitgliedschaft',\n",
    "           'fitx-vertrag',\n",
    "           'kündigeneasyfitness',\n",
    "           'proteinbedarf',\n",
    "           'fitnessland',\n",
    "           'kündigeneasyfitness',\n",
    "           'trainingspuls berechnen',\n",
    "           'pulsrechner',\n",
    "           'fitseveneleven-mitgliedschaft',\n",
    "           'alkoholabbau & promille',\n",
    "           'index.promillerechner',\n",
    "           'mitgliedschaft kündigen',\n",
    "           'promillerechner',\n",
    "           'ihr body-mass-index',\n",
    "           'bmi rechner',\n",
    "           'kalorienrechner',\n",
    "           'grundumsatz & kalorienbedarf',\n",
    "           'partnerangebote',\n",
    "           'newsletter',\n",
    "           'journalismus der presse',\n",
    "           'abonnieren',\n",
    "           '(apa)',\n",
    "           'foto:',\n",
    "           'quelle:',\n",
    "           'lesezeit:',\n",
    "           'aktuelle nachrichten',\n",
    "           'herausgegeben von',\n",
    "           'zeitung',\n",
    "           'faz.net',\n",
    "           'lesen sie mehr',\n",
    "           'stellenmarkt der sz.',\n",
    "           'bewerben sie sich jetzt',\n",
    "           'gutscheine',\n",
    "           'vergleichsportal',\n",
    "           'stern plus-inhalten',\n",
    "           'jederzeit kündbar',\n",
    "           'bereits registriert?',\n",
    "           'zur startseite',\n",
    "           'öffnet in neuem tab oder fenster',\n",
    "           'vollansicht der tabelle unter',\n",
    "           'frankfurter allgemeine zeitung', \n",
    "           'aktuelle nachrichten aus politik, wirtschaft, sport und kultur',\n",
    "           'lesezeit:'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete patterns\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: del_patterns(str(x), pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Emojis to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the presence of emojis, they are being translated into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map_emoji_to_description to translate emojis into text\n",
    "data_wo_pattern['data'] = data_wo_pattern['data'].apply(lambda x: re.sub(r'[^\\w\\s]', lambda match: map_emoji_to_description(match.group(), language = 'de',), str(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Noise\" refers to non-meaningful data, such as numbers, links, additional whitespaces, and for Spanish data, it also includes accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm_1 = data_wo_pattern.copy()\n",
    "\n",
    "# Strip_numeric -> removing digits (https://gensimr.news-r.org/reference/strip_numeric.html)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_numeric)\n",
    "\n",
    "# Strip links\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# Strip multiple whitespaces also \\n -> (https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(strip_multiple_whitespaces)\n",
    "\n",
    "# Strip spanish accents\n",
    "data_rm_1['data'] = data_rm_1['data'].apply(lambda x: remove_accents(str(x)))\n",
    "\n",
    "# reset index \n",
    "data_rm_1 = data_rm_1.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty rows \n",
    "data_rm_1 = data_rm_1.replace('', pd.NA)\n",
    "data_rm_1.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data clean 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is stored for a later treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the similiar rows (The Function is imported from utils on top)\n",
    "data_rm_1 = remove_similar_rows_per_player(data_rm_1, data_rm_1['player'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data \n",
    "data_rm_1.to_csv('../data_files/data_clean/de_clean_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess for data clean 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy data \n",
    "data_rm_2 = data_rm_1.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Create a list of stop words to remove \n",
    "stop_words_to_remove = ['nicht', 'nichts', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'anders']\n",
    "\n",
    "stop_words_to_add = ['vor', 'in', 'den', 'dem', 'beim', 'wir', 'der', 'ist','ende', 'seite', 'ersten', 'fürs', 'eh', 'blick', 'schon', 'zumal', 'erst', 'mal', 'bild', 't', '(dpa)', 'fur']\n",
    "\n",
    "# Remove the stop words to remove from the stop words list\n",
    "for word in stop_words_to_remove:\n",
    "  german_stop_words.remove(word)\n",
    "\n",
    "for word in stop_words_to_add:\n",
    "  german_stop_words.append(word)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'text' column using the apply method\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(lambda x: remove_stopwords(x, german_stop_words))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip punctuation and remove short words\n",
    "Copy of unstriped data is taken because it's needed for further data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data for third transformation\n",
    "data_rm_3 = data_rm_2.copy()\n",
    "\n",
    "# strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_punctuation)\n",
    "\n",
    "#strip_short deletes words smaller 3\n",
    "data_rm_2['data'] = data_rm_2['data'].apply(strip_short)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data clean 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data \n",
    "data_rm_2.to_csv('../data_files/data_clean/de_clean_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data condensed\n",
    "The third transformation focus on the deletion of sentences to clean the corpus. The only paragraphs kept are the one including the player names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lines and following lines where the Player name appears in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only paragraphs which include playernames \n",
    "data_with_playernames = find_lines_with_player(data_rm_3, data_rm_3['player'].unique(), n_lines = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wordgroups\n",
    "Remove first names of players and create word pairs e.g. for bayer leverkusen -> bayerleverkusen or europa league -> europaleague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform wordpair function\n",
    "data_with_playernames = name_wordgroups(data_with_playernames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete playernames from their sentences\n",
    "Because the playernames took a huge influence on the clustering they will be removed for each player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every player remove their names from the texts \n",
    "for player in data_with_playernames['player'].unique():\n",
    "    f_l_name = player.split()\n",
    "\n",
    "    # Extracting the first name\n",
    "    first_name = str(f_l_name[0])\n",
    "\n",
    "    # Extracting the last name\n",
    "    last_name = str(f_l_name[1])\n",
    "\n",
    "    updated_pattern = r\"\\b(\" + first_name.lower() + r\"|\" + last_name.lower() + r\")\\b|\"\n",
    "\n",
    "\n",
    "    # Apply the function to the data column\n",
    "    data_with_playernames.loc[data_with_playernames['player'] == player, 'data'] = data_with_playernames.loc[data_with_playernames['player'] == player, 'data'].apply(lambda x: re.sub(updated_pattern, \"\", str(x)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip punctuation and remove short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip_punctutation -> removing punctations (https://gensimr.news-r.org/reference/strip_punctuation.html)\n",
    "data_with_playernames['data'] = data_with_playernames['data'].apply(strip_punctuation)\n",
    "\n",
    "# Strip_short deletes words smaller 3\n",
    "data_with_playernames['data'] = data_with_playernames['data'].apply(strip_short)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete empty rows\n",
    "data_with_playernames = data_with_playernames.replace('', pd.NA)\n",
    "data_with_playernames.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data \n",
    "data_with_playernames.to_csv('../data_files/data_clean/de_clean_condensed.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Sentiment Preprocessing\n",
    "For the sentiment analysis we created one additional dataset where we take just the sentence where a playername appears of the clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean dataset\n",
    "df_de_sentence = pd.read_csv('../data_files/data_clean/de_clean_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep paragraph where the player name is found\n",
    "df_de_sentence = find_lines_with_player(df_de_sentence, df_de_sentence['player'].unique(),n_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentence\n",
    "df_de_sentence = extract_sentence(df_de_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy sentence column into data and remove sentence\n",
    "df_de_sentence['data']= df_de_sentence['sentence']\n",
    "df_de_sentence.drop('sentence', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove similiar sentences \n",
    "df_de_sentence = remove_similar_rows_per_player(df_de_sentence, df_de_sentence['player'].unique())\n",
    "\n",
    "# apply wordgroups\n",
    "df_de_sentence = name_wordgroups(df_de_sentence)\n",
    "\n",
    "# delete empty rows\n",
    "df_de_sentence = df_de_sentence.replace('', pd.NA)\n",
    "df_de_sentence.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data \n",
    "df_de_sentence.to_csv('../data_files/data_clean/de_clean_1_sen.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create the following CSV files:\n",
    "\n",
    "1. data1\n",
    "2. data2\n",
    "3. data_condensed\n",
    "4. de_clean_1_sen\n",
    "\n",
    "The objective of these files is to have the data cleaned and saved at different levels of detail. They allow us to use the same data for different processes with various requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps for Bayer04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the data processing, we could recommend Bayer04 to try different preprocessing combinations. By storing the data and experimenting with various combinations in subsequent processes, the goal is to achieve the best accuracy for each of the models. This iterative approach allows for fine-tuning the preprocessing steps and selecting the most effective ones that lead to improved model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
